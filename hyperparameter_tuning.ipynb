{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning using HyperDrive\n",
    "\n",
    "The first cell contains code needed to import all dependencies we will be using in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1598531914256
    }
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "from azureml.train.sklearn import SKLearn\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.parameter_expressions import uniform, normal, choice\n",
    "from azureml.core import Environment\n",
    "import shutil\n",
    "import os\n",
    "from azureml.widgets import RunDetails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The next cell contains the code we use to access the data used in this project. This dataset is external in regard to Microsoft Azure ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1598531917374
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "experiment_name = 'wine-quality-hyperdrive-experiment'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "# Note: Since the notebook for AutoML was performed first\n",
    "# we skip creation of the dataset in HyperDrive portion of Project\n",
    "# In case this is the first run notebook, please go to prerequisites.py file and run the code for dataset creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598531923519
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Hyperdrive Configuration\n",
    "\n",
    "The model we will be using is Logistic Regression and Hyperdrive will vary two parameters in random manner from defined parameter values search space. The interaction with the model is performed through estimator training script, a separate Python code that will be invoked by Hyperdrive passing by diferent combinations of parameters. Termination policy is defined as \"Bandit\" which compares best performing run and will terminate current run where the primary metric is not within the specified slack factor/slack amount. Configuration settings contain everything that defines a HyperDrive run. It includes information about parameter space sampling, termination policy, primary metric, estimator and the compute target to execute the experiment runs on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544893076
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Creating an early termination policy\n",
    "# reference: github, how-to-tune-hyperparameters\n",
    "# attribution, November 2020, https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/machine-learning/how-to-tune-hyperparameters.md\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor = 0.1, evaluation_interval = 1, delay_evaluation = 5)\n",
    "\n",
    "# Set the different params that will be used during training\n",
    "# reference: github, how-to-tune-hyperparameters\n",
    "# attribution, November 2020, https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/machine-learning/how-to-tune-hyperparameters.md\n",
    "\n",
    "param_sampling = RandomParameterSampling({\n",
    "    \"--C\": choice (10.0, 1.0, 0.1),\n",
    "    \"--max_iter\": choice(50, 100, 200)\n",
    "    })\n",
    "\n",
    "# Creating SKLearn estimator and hyperdrive config\n",
    "# resource: github, 04_hyperparameter_random_search\n",
    "# attribution, November 2020, https://github.com/microsoft/MLHyperparameterTuning/blob/master/04_Hyperparameter_Random_Search.ipynb\n",
    "compute_target = \"aml-compute\"\n",
    "estimator = SKLearn(source_directory='./scripts',\n",
    "  entry_script='train-LR.py',\n",
    "  compute_target=compute_target)\n",
    "\n",
    "# Create a HyperDriveConfig using the estimator, hyperparameter sampler, and policy.\n",
    "# resource: github, 04_hyperparameter_random_search\n",
    "# attribution, November 2020, https://github.com/microsoft/MLHyperparameterTuning/blob/master/04_Hyperparameter_Random_Search.ipynb\n",
    "\n",
    "hyperdrive_run_config = HyperDriveConfig(\n",
    "    estimator = estimator,\n",
    "    hyperparameter_sampling = param_sampling,\n",
    "    policy = early_termination_policy,\n",
    "    primary_metric_name = 'Accuracy',\n",
    "    primary_metric_goal = PrimaryMetricGoal.MAXIMIZE,\n",
    "    max_total_runs = 10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598544897941
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Submit hyperdrive run to the experiment and show run details with the widget.\n",
    "# resource: github, 04_hyperparameter_random_search\n",
    "# attribution, November 2020, https://github.com/microsoft/MLHyperparameterTuning/blob/master/04_Hyperparameter_Random_Search.ipynb\n",
    "\n",
    "from azureml.core.experiment import Experiment\n",
    "\n",
    "exp = Experiment(workspace = ws, name = 'wine-quality-tuning-hyperparameters')\n",
    "run = exp.submit(hyperdrive_run_config)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598544898497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Run Details\n",
    "\n",
    "After all the combinations of parameters are tested, it becomes obvious that variying C parameter, makes more siginificant difference in model accuracy at lower number of iterations. The C parameter defines the amount of regularization of Logistic Regression. Higher value for C results in less regularization.\n",
    "\n",
    "In the cell below, we will use the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1598546648408
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Show run detials using RunDetail widget\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "In the cell below, we will save the best model from the hyperdrive experiments and display all the properties of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "gather": {
     "logged": 1598546650307
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--C', '10', '--max_iter', '200']\n",
      "['azureml-logs/55_azureml-execution-tvmps_9fc1e7236f5b61b217ba90693d366fbd96e0e12ba8f2a8ab63d62675a6d87760_d.txt', 'azureml-logs/65_job_prep-tvmps_9fc1e7236f5b61b217ba90693d366fbd96e0e12ba8f2a8ab63d62675a6d87760_d.txt', 'azureml-logs/70_driver_log.txt', 'azureml-logs/75_job_post-tvmps_9fc1e7236f5b61b217ba90693d366fbd96e0e12ba8f2a8ab63d62675a6d87760_d.txt', 'azureml-logs/process_info.json', 'azureml-logs/process_status.json', 'logs/azureml/110_azureml.log', 'logs/azureml/dataprep/backgroundProcess.log', 'logs/azureml/dataprep/backgroundProcess_Telemetry.log', 'logs/azureml/job_prep_azureml.log', 'logs/azureml/job_release_azureml.log', 'outputs/lr-model.joblib']\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "# Get best run parameters\n",
    "# attribution, November 2020, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters\n",
    "\n",
    "best_run = run.get_best_run_by_primary_metric()\n",
    "parameter_values = best_run.get_details()['runDefinition']['arguments']\n",
    "print(best_run.get_details()['runDefinition']['arguments'])\n",
    "print(best_run.get_file_names())\n",
    "\n",
    "# Register saved best model\n",
    "\n",
    "model = best_run.register_model(model_name='Capstone-Project-LinearRegression-best', model_path='outputs/lr-model.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are two required screenshots, screenshot of the RunDetails widget that shows the progress of the training runs of the different experiments, screenshot of the best model with its run id and the different hyperparameters that were tuned.\n",
    "\n",
    "<img src=\"Screenshots\\screenshot of the RunDetails HyperParameter widget that shows the progress of the training runs of the different experiments v3.png\"/>\n",
    "\n",
    "<img src=\"Screenshots\\Screenshot of the best model with its run id and the different hyperparameters that were tuned.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model Deployment\n",
    "\n",
    "Since the AutoML model had higher accuracy, the following steps were omitted here and instead performed in AutoML notebook.\n",
    "\n",
    "TODO: In the cell below, register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, send a request to the web service you deployed to test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n/a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: In the cell below, print the logs of the web service and delete the service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n/a"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
