{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated ML\n",
    "\n",
    "The following cell contains commands to import all the dependencies for the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1613164857410
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import pkg_resources\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.dataset import Dataset\n",
    "\n",
    "from azureml.pipeline.steps import AutoMLStep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "### Overview\n",
    "In this markdown cell, we give an overview of the dataset used and the task we will be performing.\n",
    "\n",
    "The data used for training of the models is obtained from publicly UCI Machine Learning Repository. The dataset contains 1599 records of eleven red wine physicochemical properties and one output variable 'quality' as sensory data denoting perceived quality of wine according to human taste. Quality is scored from 0 to 10, latest denoting the highest quality. Classes are not balanced and there are more 'ordinary' wines than high or poor quality ones (P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis., Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009.). The data can be used both for regression and classification machine learning tasks.\n",
    "\n",
    "Our task will be to train the classification model to predict quality of unknown wine by its physicochemical properties. As starting point we will remove all missing data, as we have to be certain that we are using the clean dataset for training. \n",
    "\n",
    "The next cell contains the code we use to access the data used in this project. This dataset is external in regard to Microsoft Azure ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613164873147
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# Choose a name for experiment\n",
    "experiment_name = 'wine-quality-automl-experiment'\n",
    "\n",
    "experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "# Attempt to load the dataset from the Workspace. Otherwise,\n",
    "# Prepare Dataset from external data\n",
    "# Data located at: 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "# \n",
    "found = False\n",
    "key = \"wine-quality\"\n",
    "description_text = \"Wine Quality DataSet for Udacity Capstone Project\"\n",
    "\n",
    "if key in ws.datasets.keys(): \n",
    "        found = True\n",
    "        ds = ws.datasets[key] \n",
    "\n",
    "if not found:\n",
    "        # Create AutoML Dataset and register it into Workspace\n",
    "        web_uri = 'https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "        ds = Dataset.Tabular.from_delimited_files(web_uri, separator=';', header='ALL_FILES_HAVE_SAME_HEADERS')        \n",
    "        #Register Dataset in Workspace\n",
    "        ds = ds.register(workspace=ws,\n",
    "                                   name=key,\n",
    "                                   description=description_text)\n",
    "        \n",
    "dframe = ds.to_pandas_dataframe()\n",
    "dframe.describe()\n",
    "ds.take(5).to_pandas_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If created manually through UI then use these two code lines\n",
    "dataset_name = 'wine-quality'\n",
    "dataset = Dataset.get_by_name(workspace=ws, name=dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoML Configuration\n",
    "\n",
    "For this project we have selected classification task with accuracy as primary metric. There are many settings for AutoML experiment configuration. Reasoning for selecting certain settings may be saving resources and limiting duration of training time. Here we have limited time for training of all iterations set to 20 minutes and maximum of 5 concurrent iterations. Accuracy is most common metric for model comparison and therefore it was used here. Maximum concurrent iterations was set in order to keep within limits of compute resources available and experiment timeout was set in order to prevent overallocation of resources in case of divergent iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "gather": {
     "logged": 1613164883738
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# This section provides AutoML settings\n",
    "#\n",
    "automl_settings = {\n",
    "    \"experiment_timeout_minutes\": 20,\n",
    "    \"max_concurrent_iterations\": 5,\n",
    "    \"primary_metric\" : 'accuracy'\n",
    "                    }\n",
    "\n",
    "# The portion of AutoML config here\n",
    "#\n",
    "compute_target = \"aml-compute\"\n",
    "automl_config = AutoMLConfig(compute_target=compute_target,\n",
    "                             task = \"classification\",\n",
    "                             training_data=dataset,\n",
    "                             label_column_name=\"quality\",   \n",
    "                             path = \"./aml\",\n",
    "                             enable_early_stopping= True,\n",
    "                             featurization= 'auto',\n",
    "                             debug_log = \"automl_errors.log\",\n",
    "                             **automl_settings\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "gather": {
     "logged": 1613165208697
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on remote.\n"
     ]
    }
   ],
   "source": [
    "# Submit configured experiment\n",
    "#\n",
    "remote_run = experiment.submit(automl_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Details\n",
    "\n",
    "Azure Auto ML creates a number of pipelines in parallel and test different algorithms and parameters. Each iteration produces a model with a training score. In reality machine learning models operate under certain assumptions. One of the assumptions regards the data for model training. If the real data characteristics differs much from assumed data, then we may get a poor fit. Some models are more succeptible to overfit than others. In order to minimize the risk of overfit, we may combine several good models to get possibly an even better model, which is a technique called VotingEnsemble in case of Azure Auto ML.\n",
    "\n",
    "After submitting the experimet one of the ways to monitor progress of the experiment directly from the notebook is to use `RunDetails` widget, which gives the information like the one on captured screenshot below.\n",
    "\n",
    "![](https://github.com/DivkovicD/ML-Engineer-w-MS-Azure/blob/master/Screenshots/Screenshot%20of%20RunDetails%20widget%20showing%20the%20progress%20of%20training%20runs%20of%20different%20experiments%20v5.png?raw=true)\n",
    "\n",
    "In the cell below, we used the `RunDetails` widget to show the different experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613165217940
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different models perform differently in regard to primary metric chosen. AutoML calculates performance metrics, based on the scikit learn implementation for each classification model generated for experiment. Common consideration with all models is class imbalance. Models that are more sensitive to class imbalance perform show less accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model\n",
    "\n",
    "The following cell contains the code to get the best model from AutoML experiments and displays properties of the model. Then we save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1613166551605
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve and save your best automl model.\n",
    "# attribution, November 2020, https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python\n",
    "#\n",
    "best_run, fitted_model = remote_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of AutoML run was VotingEnsemble model which is a combined performance of best performing models from AutoML experiment run. The ensemble was created from previous AutoML iterations with soft voting. The VotingEnsemble consists of ['XGBoostClassifier', 'KNN', 'LightGBM', 'XGBoostClassifier', 'XGBoostClassifier', 'LightGBM', 'LightGBM', 'XGBoostClassifier', 'LightGBM', 'SVM', 'ExtremeRandomTrees', 'LightGBM'] algorithms, which are top twelve models rated by accuracy. The AutoML Voting Ensemble selected parameters read from azureml-logs:\n",
    "\n",
    "- 'ensemble_iterations': 35, 27, 0, 50, 1, 39, 44, 45, 8, 7, 28, 31\n",
    "- 'training_type': 'MeanCrossValidation'\n",
    "- 'goal': 'accuracy_max'\n",
    "- 'primary_metric': 'accuracy'\n",
    "\n",
    "Other AutoML parameters were mostly default values. The detailed list of parameters can also be obtained from Raw JSON file located under \"See all properties\" of Details blade of the AutoML experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "gather": {
     "logged": 1613166552265
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs/automl-wine-quality-model.pkl']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the best model\n",
    "# Example of approach at https://benalexkeen.com/using-azure-automl-and-aml-for-assessing-multiple-models-and-deployment/\n",
    "# folder 'outputs' must be present\n",
    "#\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "model_path = 'outputs/automl-wine-quality-model.pkl'\n",
    "joblib.dump(fitted_model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Screenshot of the best model with its run id:\n",
    "\n",
    "![](https://github.com/DivkovicD/ML-Engineer-w-MS-Azure/blob/master/Screenshots/Screenshot%20of%20the%20best%20model%20(AutoML)%20with%20its%20run%20id.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "\n",
    "The result of AutoML run was VotingEnsemble model which achieved higher accuracy in comparison with Hyperparameter tuning approach. The result of VotingEnsemble model is a combined performance of best performing models from AutoML experiment run. Therefore we decide to deploy this model, so the next cell contains the code to register the model, create an inference config and deploy the model as a web service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the best model that was previously saved\n",
    "#\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "from azureml.core.run import Run\n",
    "\n",
    "model = Model.register(model_path = \"outputs/automl-wine-quality-model.pkl\",\n",
    "                       model_name = \"automl-wine-quality-model\",\n",
    "                       tags = {\"VotingEnsemble\": \"1.0\"},\n",
    "                       description = \"AutoML model for prediction of wine quality\",\n",
    "                       workspace = ws)\n",
    "\n",
    "print(model.name, model.id, model.version, sep = '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inference configuration\n",
    "#\n",
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "\n",
    "env = Environment.get(ws, \"AzureML-AutoML\")\n",
    "# sa kraja prethodnog reda: .clone(env_name)\n",
    "\n",
    "for pip_package in [\"scikit-learn\"]:\n",
    "    env.python.conda_dependencies.add_pip_package(pip_package)\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='scoring2.py',\n",
    "                                    environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running............................................\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "http://90199394-1357-4556-9c4c-d9c4c8f7e9f1.southcentralus.azurecontainer.io/score\n"
     ]
    }
   ],
   "source": [
    "# Setting deployment configuration\n",
    "#\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "\n",
    "# Provide model, inference, deployment configuration, web service name and location to deploy the model\n",
    "service = Model.deploy(\n",
    "    workspace = ws,\n",
    "    name = \"wine-quality-web-service\",\n",
    "    models = [model],\n",
    "    inference_config = inference_config,\n",
    "    deployment_config = deployment_config)\n",
    "\n",
    "\n",
    "service.wait_for_deployment(show_output=True)\n",
    "# Record and use the following output for interaction with deployed service\n",
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A screenshot showing the model endpoint as active:\n",
    "\n",
    "![](https://github.com/DivkovicD/ML-Engineer-w-MS-Azure/blob/master/Screenshots/Screenshot%20showing%20model%20endpoint%20as%20active%20-%204%20automl%20notebook.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment lines below for enabling Application Inisghts and run the cell\n",
    "# in this case it is necessary to use Python script score-w-appinsig.py instead of scoring2.py\n",
    "# service.update(enable_app_insights=True)\n",
    "# service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598431657736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "The cell below contains the code that sends a request to deployed web service. There are several ways to do it. The first demonstrates simple JSON formating without error catching, the second contains the sample code from Consume blade of deployed model, and third shows how could it be done via `curl` commandÂ¸."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "gather": {
     "logged": 1613169567929
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6]\n",
      "[5, 6]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from ast import literal_eval\n",
    "    \n",
    "# URL for the web service from scoring_uri of the deployed model\n",
    "scoring_uri = 'http://90199394-1357-4556-9c4c-d9c4c8f7e9f1.southcentralus.azurecontainer.io/score'\n",
    "## If the service is authenticated, set the key or token and uncomment the line below\n",
    "# key = '<your key or token>'\n",
    "    \n",
    "# Two sets of data to score, two results back\n",
    "data = {\"data\":\n",
    "        [\n",
    "            {\"fixed acidity\": 7.4,\n",
    "             \"volatile acidity\": 0.7,\n",
    "             \"citric acid\": 0,\n",
    "             \"residual sugar\": 1.9,\n",
    "             \"chlorides\": 0.076,\n",
    "             \"free sulfur dioxide\": 11,\n",
    "             \"total sulfur dioxide\": 34,\n",
    "             \"density\": 0.9978,\n",
    "             \"pH\": 3.51,\n",
    "             \"sulphates\": 0.56,\n",
    "             \"alcohol\": 9.4\n",
    "          },\n",
    "            {\"fixed acidity\": 11.2,\n",
    "             \"volatile acidity\": 0.28,\n",
    "             \"citric acid\": 0.56,\n",
    "             \"residual sugar\": 1.9,\n",
    "             \"chlorides\": 0.075,\n",
    "             \"free sulfur dioxide\": 17,\n",
    "             \"total sulfur dioxide\": 60,\n",
    "             \"density\": 0.998,\n",
    "             \"pH\": 3.16,\n",
    "             \"sulphates\": 0.58,\n",
    "             \"alcohol\": 9.4\n",
    "          }\n",
    "        ]\n",
    "     }\n",
    "\n",
    "# Convert to JSON string\n",
    "input_data = json.dumps(data)\n",
    "    \n",
    "# Set appropriate content type\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "## Note: If authentication is enabled, set the authorization header, by uncommenting the next line of code\n",
    "# headers['Authorization'] = f'Bearer {key}'\n",
    "    \n",
    "# Post the formated request and display the response\n",
    "response = requests.post(scoring_uri, input_data, headers=headers)\n",
    "print(response.text)\n",
    "# And alterntive way to display result    \n",
    "result = literal_eval(response.text)\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': [{'fixed acidity': 7.4, 'volatile acidity': 0.7, 'citric acid': 0, 'residual sugar': 1.9, 'chlorides': 0.076, 'free sulfur dioxide': 11, 'total sulfur dioxide': 34, 'density': 0.9978, 'pH': 3.51, 'sulphates': 0.56, 'alcohol': 9.4}]}\n",
      "b'{\"data\": [{\"fixed acidity\": 7.4, \"volatile acidity\": 0.7, \"citric acid\": 0, \"residual sugar\": 1.9, \"chlorides\": 0.076, \"free sulfur dioxide\": 11, \"total sulfur dioxide\": 34, \"density\": 0.9978, \"pH\": 3.51, \"sulphates\": 0.56, \"alcohol\": 9.4}]}'\n",
      "b'[5]'\n"
     ]
    }
   ],
   "source": [
    "# Taken from Consume blade of deployed model\n",
    "#\n",
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "data = {\n",
    "    \"data\":\n",
    "    [\n",
    "        {\n",
    "            'fixed acidity': 7.4,\n",
    "            'fixed acidity': 7.4,\n",
    "            'volatile acidity': 0.7,\n",
    "            'citric acid': 0,\n",
    "            'residual sugar': 1.9,\n",
    "            'chlorides': 0.076,\n",
    "            'free sulfur dioxide': 11,\n",
    "            'total sulfur dioxide': 34,\n",
    "            'density': 0.9978,\n",
    "            'pH': 3.51,\n",
    "            'sulphates': 0.56,\n",
    "            'alcohol': 9.4\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "print(data)\n",
    "body = str.encode(json.dumps(data))\n",
    "print(body)\n",
    "url = 'http://683e48e7-68d3-4d83-a9d7-fd74131b3b04.southcentralus.azurecontainer.io/score'\n",
    "api_key = '' # Replace this with the API key for the web service and uncomment the following line, while commenting the next one\n",
    "# headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(json.loads(error.read().decode(\"utf8\", 'ignore')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "gather": {
     "logged": 1613168320796
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]"
     ]
    }
   ],
   "source": [
    "# Substitute the last line of the cell with scoring_uri for interaction with deployed service\n",
    "#\n",
    "from azureml.core.authentication import InteractiveLoginAuthentication\n",
    "\n",
    "interactive_auth = InteractiveLoginAuthentication()\n",
    "auth_header = interactive_auth.get_authentication_header()\n",
    "\n",
    "!curl -X POST \\\n",
    "    -H 'Content-Type':'application/json' \\\n",
    "    -d '{\"data\":[{\"fixed acidity\": 7.4, \"volatile acidity\": 0.7, \\\n",
    "        \"citric acid\": 0, \"residual sugar\": 1.9, \"chlorides\": 0.076, \"free sulfur dioxide\": 11, \\\n",
    "        \"total sulfur dioxide\": 34, \"density\": 0.9978, \"pH\": 3.51, \"sulphates\": 0.56, \"alcohol\": 9.4}]}' \\\n",
    "    http://90199394-1357-4556-9c4c-d9c4c8f7e9f1.southcentralus.azurecontainer.io/score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "gather": {
     "logged": 1598432765711
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "The following cell prints out web service logs, deletes the service and comupte target, therefore releasing the resources in order to economize with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Get logs and release resources\n",
    "#\n",
    "print(service.get_logs())\n",
    "service.delete()\n",
    "compute_target.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The cells below are helper files to be used in case original files are altered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%writefile scoring2.py\n",
    "\n",
    "# Reccomended approach\n",
    "# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-and-where?tabs=python\n",
    "#\n",
    "# Inspiration for portions of code from https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-advanced-entry-script\n",
    "# and https://docs.microsoft.com/en-us/azure/machine-learning/tutorial-power-bi-custom-model\n",
    "#\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "from azureml.core.model import Model\n",
    "\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n",
    "\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    path = os.getenv('AZUREML_MODEL_DIR') \n",
    "    model_path = os.path.join(path, 'automl-wine-quality-model.pkl')\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "\n",
    "input_sample = pd.DataFrame(data=[{\n",
    "            \"fixed acidity\": 7.4,\n",
    "             \"volatile acidity\": 0.7,\n",
    "             \"citric acid\": 0,\n",
    "             \"residual sugar\": 1.9,\n",
    "             \"chlorides\": 0.076,\n",
    "             \"free sulfur dioxide\": 11,\n",
    "             \"total sulfur dioxide\": 34,\n",
    "             \"density\": 0.9978,\n",
    "             \"pH\": 3.51,\n",
    "             \"sulphates\": 0.56,\n",
    "             \"alcohol\": 9.4\n",
    "}])\n",
    "\n",
    "# Expected result is inetger.\n",
    "output_sample = np.array([0])\n",
    "\n",
    "@input_schema('data', PandasParameterType(input_sample))\n",
    "@output_schema(NumpyParameterType(output_sample))\n",
    "\n",
    "def run(data):\n",
    "    try:\n",
    "        print(\"Inputs:\")\n",
    "        print(data.columns)\n",
    "        print(type(data))\n",
    "        result = model.predict(data)\n",
    "        print(\"Result:\")\n",
    "        print(result)\n",
    "        return result.tolist()\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
